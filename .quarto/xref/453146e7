{"entries":[{"caption":"Memory reduction of FlashAttention over standard PyTorch attention implementation at different sequence lengths.","key":"fig-flashattn-memory","order":{"number":6,"section":[2,1,0,0,0,0,0]}},{"caption":"Comparison of wallclock time needed for each operation in PyTorch implementation of vanilla attention and FlashAttention. Figure taken from Tri Dao et al. [@flashattention]","key":"fig-wallclock","order":{"number":2,"section":[0,1,0,0,0,0,0]}},{"caption":"Wallclock-time speedup of FlashAttention over standard PyTorch attention implementation at different sequence lengths on NVIDIA A100. Figures taken from Tri Dao et al. [@flashattention].","key":"fig-wallclock-2","order":{"number":7,"section":[2,1,0,0,0,0,0]}},{"caption":"Pseudocode of online softmax algorithm","key":"fig-online-softmax","order":{"number":5,"section":[1,0,0,0,0,0,0]}},{"caption":"Overview of FlashAttention algorithm. Figure taken from Tri Dao et. al. [@flashattention]","key":"fig-flashattention","order":{"number":3,"section":[0,4,0,0,0,0,0]}},{"caption":"Pseudocode of safe softmax algorithm.","key":"fig-safe-softmax","order":{"number":4,"section":[1,0,0,0,0,0,0]}},{"caption":"GPU memory hierarchy. Relative to SRAM, HBM is slower but has more memory","key":"fig-gpu","order":{"number":1,"section":[0,1,0,0,0,0,0]}}],"headings":["introduction","related-work","vanilla-attention","flash-attention","tiling","recomputation","experimental-results","personal-comment"]}
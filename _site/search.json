[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rodi Düger",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nHempel’s (Raven) Paradox: An Introduction\n\n\n\n\n\n\nphilosophy\n\n\n\n\n\n\n\n\n\nDec 31, 2024\n\n\nRodi Düger\n\n\n\n\n\n\n\n\n\n\n\n\nFlash Attention: A Brief Overview\n\n\n\n\n\n\ncomputer_science\n\n\nmachine_learning\n\n\n\n\n\n\n\n\n\nAug 2, 2024\n\n\nRodi Düger\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding RSA: Number Theory,Primes, Fermat, Euler and such\n\n\n\n\n\n\ncomputer_science\n\n\n\n\n\n\n\n\n\nAug 28, 2023\n\n\nRodi Düger\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/understanding-rsa/index.html",
    "href": "posts/understanding-rsa/index.html",
    "title": "Understanding RSA: Number Theory,Primes, Fermat, Euler and such",
    "section": "",
    "text": "While searching the internet for a somehow in-depth article on the mathematics underpinning RSA encryption, I found myself somewhat dissatisfied with the available resources. This ignited a longstanding aspiration within me to start writing, so here is my debut piece !"
  },
  {
    "objectID": "posts/understanding-rsa/index.html#chinese-remainder-theorem",
    "href": "posts/understanding-rsa/index.html#chinese-remainder-theorem",
    "title": "Understanding RSA: Number Theory,Primes, Fermat, Euler and such",
    "section": "Chinese Remainder Theorem",
    "text": "Chinese Remainder Theorem\nThe Chinese Remainder Theorem (CRT) is a fundamental theorem in number theory that deals with solving systems of simultaneous modular congruences. It provides a way to find a unique solution to a set of congruences when the moduli involved are pairwise coprime (i.e., they have no common factors other than 1). The theorem is named after its historical association with ancient Chinese mathematics.\nTheorem:\nLet \\(n_1, n_2, ..., n_k\\) be pairwise coprime positive integers and let \\(a_1,a_2,...,a_k\\) be any set of integers. Then the system of modular congruences\n\\[\n\\displaylines{x \\equiv a_1 \\;(mod \\;n_1)\\\\x \\equiv a_2 \\;(mod \\; n_2)\\\\.\\\\.\\\\x \\equiv \\; a_k \\; (mod \\; n_k) }\n\\]\nhas a unique solution in \\(mod \\; N\\), whereas \\(N:= n_1 \\cdot n_2 \\cdot...\\cdot n_k\\)\nEssentially, what the thorem states is that, the map\n\\[\nx \\; mod \\; N \\mapsto (x \\; mod \\; n_1,x \\; mod \\; n_2,...,x \\; mod \\; n_k )\n\\]\ndefines an isomorphism between \\(\\mathbb{Z}/N\\mathbb{Z}\\) and \\(\\mathbb{Z}/n_1\\mathbb{Z}\\times\\mathbb{Z}/n_2\\mathbb{Z}\\times...\\times\\mathbb{Z}/n_k\\mathbb{Z}\\)\nI’m not going to delve into a formal proof explaining why Euler’s totient function acts as a multiplicative function under certain assumptions. However, what’s essential to recognize here is that the isomorphism between these two rings leads to the pivotal observation of Euler’s totient function possessing a multiplicative property.\nThere’s another observation we can make about Euler’s totient function, and it will come in handy as we proceed:\n\nLet \\(p\\) be a prime and \\(k \\geq1\\). Then the following equation holds:\n\\[\n\\phi(p^k) = p^k - p^{k-1}\n\\]\nProof: Since \\(p\\) is a prime number, the only possible values of \\(gcd(p^k, m)\\) are \\(1, p, p^2, ..., p^k\\), and the only way to have \\(gcd(p^k, m) &gt; 1\\) is if \\(m\\) is a multiple of \\(p\\), that is, \\(m \\in {p, 2p, 3p, ..., p^{k − 1} \\cdot p = p^k}\\), and there are \\(p^{k − 1}\\) such multiples not greater than \\(p^k\\). Therefore, the other \\(p^k − p^{k − 1}\\) numbers are all relatively prime to \\(p^k\\)².\nExample: Let’s compute \\(\\phi(3^2)\\):\n\\[\n\\displaylines{gcd(0,9) = 9 \\;\\; ,\\;\\; gcd(1,9)=1  \\;\\; ,\\;\\;  gcd(2,9) = 1 \\\\ gcd (3,9) = 3  \\;\\; ,\\;\\;  gcd(4,9) = 1 \\;\\; ,\\;\\; gcd(5,9) = 1 \\\\gcd(6,9) = 3 \\;\\; ,\\;\\; gcd(7,9)=1  \\;\\; ,\\;\\;  gcd(8,9) = 1\\\\ \\implies \\phi(3^2) = 6 = 3^2 - 3^1}\n\\]\n\nNow let’s take a look at the simpler approach to compute this function for any integer:\n\nLet \\(m\\) has the following prime factorization:\n\\[\nm = p_1^{e_1} \\cdot p_2^{e_2}\\cdot...\\cdot p_n^{e_n}\n\\]\nwhereas \\(e_i \\in \\mathbb{N}\\). With a little assistance from our previous observation, we can assert the following:\n\\[\n\\phi(m) = \\prod_{i=1}^n\\phi(p_i^{e^i}) = \\prod_{i=1}^n(p_i^{e_i} - p_i^{e_i -1})\n\\]\n\nNow it should be more clear how we computed \\(\\phi(n)\\) as \\(\\phi(n) = \\phi(p) \\cdot \\phi(q)\\) in the key generation section.\nPrime factorization of \\(n\\) is \\(n = p^1 \\cdot q^1\\), then:\n\\[\n\\phi(n) = \\phi(p^1 \\cdot q^1) = \\phi(p^1) \\cdot \\phi(q^1) = (p^1-p^0)\\cdot(q^1-q^0) = (p-1)\\cdot(q-1)\n\\]\nThis insight should shed light on why we generate prime numbers to generate keys in the first place. It’s crucial to emphasize that this computation becomes feasible only when we possess the prime factorization of a given integer \\(n\\). Yet, for a large \\(n\\), it might prove impractical to factorize it into its prime components within a reasonable timeframe. Exactly for this reason, we emphasize that “RSA uses the complexity of prime factorization to guarantee its security,” as this very characteristic forms the heart of the RSA cryptosystem.\nIf we’re aware of the prime factors of a specific number \\(n\\), we can efficiently calculate the Euler’s totient function \\(\\phi(n)\\), allowing for efficient decryption. However, when we lack knowledge of the prime factors of \\(n\\) efficiently computing \\(\\phi(n)\\) becomes a daunting task. We could resort to a brute-force approach, yet this becomes unfeasible when dealing with significantly large values of \\(n\\), because of the immense time it would demand.\n\nNow that we’ve understood some key features of Euler’s totient function that are vital for key generation, let’s address the question: “Why do we even use Euler’s totient function?”\nLet’s explore the answer."
  },
  {
    "objectID": "posts/flash-attention/index.html",
    "href": "posts/flash-attention/index.html",
    "title": "Flash Attention: A Brief Overview",
    "section": "",
    "text": "Transformer architecture (Vaswani et al. 2017) has been a milestone for many deep learning application areas, particularly in NLP domain as the backbone of most large language models (LLMs). Scaling up these models has been the key factor allowing them to achieve their high levels of performance and capabilities (Kaplan et al. 2020; Hoffmann et al. 2022). As the models grow larger, trained on more data with increased computational resources, they are able to learn more comprehensive patterns and representations, leading to improvements in understanding and generating human language as well as solving complex tasks (Wei et al. 2022).\nThe core component of the Transformer architecture is the attention mechanism, which allows embeddings to incorporate contextual information. The standard implementation of the attention mechanism is slow due to its quadratic time and memory complexity and hence becomes a computational bottleneck, especially for long sequences. As a consequence, a primary challenge with scaling up these models is efficiency.\nTo address this efficiency problem, FlashAttention (Dao et al. 2022) has been proposed as an exact IO-aware attention algorithm. Rather than focusing on reducing the computation of the attention algorithm, FlashAttention reduces the number of IO operations between the GPU’s relatively slow high-bandwidth memory (HBM) and fast on-chip SRAM and effectively utilizes the asymmetric memory hierarchy in graphics processing units (GPUs).\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: GPU memory hierarchy. Relative to SRAM, HBM is slower but has more memory\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Comparison of wallclock time needed for each operation in PyTorch implementation of vanilla attention and FlashAttention. Figure taken from Tri Dao et al. (Dao et al. 2022)"
  },
  {
    "objectID": "posts/flash-attention/index.html#introduction",
    "href": "posts/flash-attention/index.html#introduction",
    "title": "Flash Attention: A Brief Overview",
    "section": "",
    "text": "Transformer architecture (Vaswani et al. 2017) has been a milestone for many deep learning application areas, particularly in NLP domain as the backbone of most large language models (LLMs). Scaling up these models has been the key factor allowing them to achieve their high levels of performance and capabilities (Kaplan et al. 2020; Hoffmann et al. 2022). As the models grow larger, trained on more data with increased computational resources, they are able to learn more comprehensive patterns and representations, leading to improvements in understanding and generating human language as well as solving complex tasks (Wei et al. 2022).\nThe core component of the Transformer architecture is the attention mechanism, which allows embeddings to incorporate contextual information. The standard implementation of the attention mechanism is slow due to its quadratic time and memory complexity and hence becomes a computational bottleneck, especially for long sequences. As a consequence, a primary challenge with scaling up these models is efficiency.\nTo address this efficiency problem, FlashAttention (Dao et al. 2022) has been proposed as an exact IO-aware attention algorithm. Rather than focusing on reducing the computation of the attention algorithm, FlashAttention reduces the number of IO operations between the GPU’s relatively slow high-bandwidth memory (HBM) and fast on-chip SRAM and effectively utilizes the asymmetric memory hierarchy in graphics processing units (GPUs).\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: GPU memory hierarchy. Relative to SRAM, HBM is slower but has more memory\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Comparison of wallclock time needed for each operation in PyTorch implementation of vanilla attention and FlashAttention. Figure taken from Tri Dao et al. (Dao et al. 2022)"
  },
  {
    "objectID": "posts/flash-attention/index.html#related-work",
    "href": "posts/flash-attention/index.html#related-work",
    "title": "Flash Attention: A Brief Overview",
    "section": "Related Work",
    "text": "Related Work\nMany approaches have been proposed to address the quadratic time and memory complexity of Transformers and to scale them to longer sequences. Some of these approaches include low-rank approximation (Choromanski et al. 2021; Likhosherstov et al. 2021), sparse approximation (Kitaev, Kaiser, and Levskaya 2020), a combination of both (Beltagy, Peters, and Cohan 2020), and compression along the sequence (Dai et al. 2019). Most of these methods offer a trade-off between efficiency and accuracy. FlashAttention distinguishes itself among these methods as an efficient and exact alternative."
  },
  {
    "objectID": "posts/flash-attention/index.html#vanilla-attention",
    "href": "posts/flash-attention/index.html#vanilla-attention",
    "title": "Flash Attention: A Brief Overview",
    "section": "Vanilla Attention",
    "text": "Vanilla Attention\nIn its simplified form without the scaling factor before applying the softmax, the vanilla attention computation can be written as\n\\[\nO = \\text{softmax}(QK^T)V,\n\\]\nwhere $O,Q,K,V ^{N d} $. The vanilla attention algorithm computes the output as following:\n\nLoad \\(Q\\) and \\(K\\) by blocks from HBM to SRAM | IO operation !\nCompute the intermediate result \\(S_0 = QK^T\\in \\mathbb{R}^{N \\times N}\\)\nWrite the intermediate result \\(S_0\\) to HBM | IO operation !\nLoad *\\(S_0\\) from HBM to SRAM | IO operation !\nApply softmax to \\(S_0\\) along the second dimension, which results in the intermediate result \\(S_1 = \\text{softmax}(S_0) \\in \\mathbb{R}^{N \\times N}\\)\nWrite \\(S_1\\) to HBM | IO operation !\nLoad \\(S_1\\) and \\(V\\) by blocks from HBM to SRAM\nCompute the output \\(O = S_1V\\)\nWrite \\(O\\) to HBM | IO operation !\n\nEven in its most simplified form, attention computation requires data to move between HBM and SRAM several times due to the limited capacity of SRAM, which is e.g. approximately 20 MB in the NVIDIA A100, given it contains 108 streaming multiprocessors each equipped with 192 KB of SRAM (NVIDIA A100 Tensor Core GPU Architecture 2020). Additional memory-bound operations, such as masking and dropout, also increase the IO overhead of the computation.\nThis demonstrates that the vanilla attention algorithm does not account for the cost of HBM reads and writes, making it IO-unaware. FlashAttention addresses this problem."
  },
  {
    "objectID": "posts/flash-attention/index.html#flash-attention",
    "href": "posts/flash-attention/index.html#flash-attention",
    "title": "Flash Attention: A Brief Overview",
    "section": "Flash Attention",
    "text": "Flash Attention\nIn contrast to the vanilla attention algorithm, FlashAttention computes exact attention with fewer HBM reads and writes. It achieves this by applying two well-established optimization techniques: tiling and recomputation. The key idea behind FlashAttention is to avoid materializing intermediate matrices and to fuse all CUDA kernels (matrix multiplication, softmax etc.) used in the vanilla attention computation into one as depicted in Figure 3.\n\n\n\n\n\n\nFigure 3: Overview of FlashAttention algorithm. Figure taken from Tri Dao et. al. (Dao et al. 2022)"
  },
  {
    "objectID": "posts/flash-attention/index.html#experimental-results",
    "href": "posts/flash-attention/index.html#experimental-results",
    "title": "Flash Attention: A Brief Overview",
    "section": "Experimental Results",
    "text": "Experimental Results\nIn this section, we analyze the experimental results of the FlashAttention.@fig-flashattn_memory demonstrates the reduction in HBM memory usage compared to the vanilla attention algorithm. Since FlashAttention does not materialize the \\(N \\times N\\) intermediate matrices, it only requires \\(O(N)\\) additional HBM memory for the output and softmax statistics as opposed to \\(O(N^2)\\) memory requirement of the vanilla attention algorithm. This results in a quadratic increase in memory reduction with respect to the sequence length \\(N\\).\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Memory reduction of FlashAttention over standard PyTorch attention implementation at different sequence lengths.\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Wallclock-time speedup of FlashAttention over standard PyTorch attention implementation at different sequence lengths on NVIDIA A100. Figures taken from Tri Dao et al. (Dao et al. 2022).\n\n\n\n\n\n\nIn addition to memory reduction, FlashAttention is also faster compared to the vanilla attention algorithm as depicted in Figure 7. The speedup is particularly significant when optional dropout and masking operations are applied during the attention computation. This behavior is expected, as the optimizations employed in FlashAttention aim to reduce the I/O complexity of the vanilla attention algorithm. Memory-bound operations, such as masking and dropout, are the primary sources of bottlenecks in terms of wall clock time."
  },
  {
    "objectID": "posts/flash-attention/index.html#personal-comment",
    "href": "posts/flash-attention/index.html#personal-comment",
    "title": "Flash Attention: A Brief Overview",
    "section": "Personal Comment",
    "text": "Personal Comment\n“Attention is All You Need” (Vaswani et al. 2017) in 2017 marked a pivotal moment, establishing the Transformer architecture and attention mechanism as fundamental building blocks for many groundbreaking research endeavors and widely-used products. What, I find particularly interesting about FlashAttention is how, despite several years of research in one of the most rapidly evolving scientific domains, such an elegant yet “simple” line of optimization could be overlooked for arguably one of the most crucial operations. This is especially surprising given the significant financial incentives and vast resources available to companies that would benefit from an algorithm such as FlashAttention. Of course, hindsight is 20/20.\nBy the way, FlashAttention 2 (Dao 2024) and FlashAttention 3 (Shah et al. 2024) are available as even more optimized attention kernels, and their adoption has been widespread across the industry. In that regard, I have become a big fan of Tri Dao’s research. I understand that academic and industrial research are driven by different motivations, but I believe more academics should pay attention to real-world use cases and their computational constraints."
  },
  {
    "objectID": "posts/raven-paradox/index.html",
    "href": "posts/raven-paradox/index.html",
    "title": "Hempel’s (Raven) Paradox: An Introduction",
    "section": "",
    "text": "The Raven Paradox, also known as Hempel’s Paradox, is a paradox introduced by Carl Gustav Hempel (C. G. Hempel 1937; Carl G. Hempel 1945) that explores the notion of qualitative confirmation—specifically, what constitutes evidence for a particular hypothesis.\nStatements with empirical content require validation through reference to the empirical reality around us. However, it is not possible to conclusively verify certain statements, such as universally quantified ones, by means of deductive logic. The sentence “All ravens are black” is an example of a universally quantified statement. No finite amount of observation could verify this statement, as it asserts validity for an infinite number of instances. Although it is not possible to conclusively verify such general hypotheses through observation and deductive logic, it can intuitively be said that each observation of a “black raven” provides evidence for the hypothesis and confirms it. In this sense, the non-deductive relationship between evidence and hypothesis is crucial for scientific practice, as science largely deals with such general hypotheses. Nevertheless, attempts to formalize an explicit theory of confirmation/disconfirmation encounter problems, including Hempel’s Raven Paradox. To fully grasp why Hempel formulated this paradox, it is important to understand the historical and philosophical context in which it emerged.\nHempel was a pioneering figure within the Logical Positivism movement, contributing greatly to its refinement and establishment within philosophical circles, and the paradox should be understood within this context. Logical Positivism was a philosophical movement that flourished in the 1920s and 1930s in central Europe, particularly among members of the Vienna Circle. It is characterized by an approach to philosophy, grounded in the belief that meaningful statements are those that can be empirically verified or are tautological (true by definition). This principle, known as the verifiability criterion of meaningfulness, was central to their efforts to distinguish “meaningful” questions—those that can be the subject of philosophical and scientific endeavour—from “meaningless” ones, which lack empirical content or logical form and thus fall outside the realm of legitimate discourse. By this standard, sentences that are neither tautological nor empirically verifiable, such as theological or metaphysical claims, were considered meaningless. Logical Positivists believed that scientific methodology, with its emphasis on empirical testing and observation, was the ideal way to acquire knowledge. They argued that the sole purpose of philosophy should be to clarify the language of science, ensuring that all philosophical discourse is grounded in empirical reality and logical analysis. However, while the verifiability criterion of meaningfulness ruled out statements without empirical content as “meaningless,” it also classified many universally quantified scientific statements, such as physical laws and theories, as “meaningless.”, which ultimately challenged the very foundation of scientific research."
  },
  {
    "objectID": "posts/raven-paradox/index.html#hempels-resolution",
    "href": "posts/raven-paradox/index.html#hempels-resolution",
    "title": "Hempel’s (Raven) Paradox: An Introduction",
    "section": "Hempel’s Resolution",
    "text": "Hempel’s Resolution\nHempel himself, denied that the conclusion is paradoxical. He argued, that “impression of a paradoxical situation is not objectively founded; it is a psychological illusion” (Carl G. Hempel 1945). He argumented, that a general hypothesis such as “All ravens are black” is in fact a constraint on all objects, and not just on ravens. Furthermore, he pointed out that when discussing how evidence \\(E\\) supports a hypothesis \\(H\\), we implicitly rely on a set of background information \\(B\\). He suggested that the paradoxical conclusion would disappear if we did not factor in this background knowledge.\nTo elaborate on Hempel’s first point, the hypothesis “All ravens are black” can be seen not merely as a statement about ravens but as a universal constraint on all objects. In other words, the hypothesis asserts that if any object is a raven, then it must also be black. This means that the hypothesis implicitly makes a claim about every possible object in the world: any non-black object cannot be a raven. From this perspective, observing a non-black object, such as a white shoe, is relevant to the hypothesis because it confirms that the object in question is not a counterexample to the rule. While it may feel counterintuitive, Hempel’s view is that confirming the absence of non-black ravens is just as valid as confirming the presence of black ones.\nRegarding the second point, Hempel stresses the role of background information in how evidence is interpreted. When we assess how a particular piece of evidence supports a hypothesis, we do so with additional background assumptions that often go unnoticed. For instance, if we already know that most objects around us are not ravens, observing a white shoe feels irrelevant to the hypothesis “All ravens are black.” However, if we imagine a scenario where the distribution of ravens and non-ravens is unknown, the observation of any non-black, non-raven object gains more significance. Hempel argued that the sense of paradox arises because we intuitively operate with background information, like knowing what ravens look like and their rarity, without realizing its influence on our reasoning. When this background information is accounted for, the seemingly paradoxical nature of the conclusion dissolves.\nHempel’s key insight is that confirmation should be seen as a three-way relationship: “evidence \\(E\\) supports the hypothesis \\(H\\) relative to background information \\(B\\),” rather than just a two-way link between \\(E\\) and \\(H\\). However, Hempel’s theory of confirmation does not offer a framework to systematically account for background information \\(B\\) in order to “isolate” the confirmational impact of evidence \\(E\\) on its own. (Fitelson and Hawthorne 2010) In other words, it does not make a distinction between “\\(E\\) confirms \\(H\\) given \\(B\\)” and “\\(E \\And B\\) confirms \\(H\\)”. In contrast, Bayesian confirmation theory offers a more structured approach. Using Bayes’ Theorem, it quantifies how much evidence \\(E\\) raises the probability of the hypothesis \\(H\\) in light of background information \\(B\\). While Bayesian confirmation is fundamentally quantitative, it can easily be interpreted qualitatively. So, a set of alternative approaches for resolving the Raven Paradox can be found by looking at the Bayesian view on confirmation."
  },
  {
    "objectID": "posts/raven-paradox/index.html#bayesian-approach",
    "href": "posts/raven-paradox/index.html#bayesian-approach",
    "title": "Hempel’s (Raven) Paradox: An Introduction",
    "section": "Bayesian Approach",
    "text": "Bayesian Approach\nBayesianism is basically a probabilistic framework, which in simple words describe how to update the belief regarding the likelihood of a hypothesis in light of new evidence. Bayes’ Theorem explicates this as following:\n\\[\nP(H|E) = \\frac{P(E|H) \\cdot P(H)}{P(E)}\n\\]\nWhere:\n\n\\(P(H|E)\\) is the posterior probability: the probability of the hypothesis \\(H\\) after observing evidence \\(E\\).\n$P(E|H) $ is the likelihood: the probability of observing evidence \\(E\\) given that \\(H\\) is true.\n\\(P(H)\\) is the prior probability: the initial probability of \\(H\\) before observing any evidence.\n\\(P(E)\\) is the marginal likelihood: the overall probability of observing ( E ) under all possible hypotheses. It can also be written as \\(P(E) = P(D|H) \\cdot P(H) + P(D|\\neg H) \\cdot P(\\neg H)\\)\n\nIt is important to point out that the Bayesian formulation does not presuppose that the probabilities correspond to the objective likelihood of events—assuming such a thing as ‘objective probability’ even exists. Instead, these probabilities represent our degrees of belief or confidence in certain outcomes based on the available information. This subjective interpretation of probability is central to Bayesianism, as it allows us to update our beliefs dynamically in response to new evidence. In this regard, Bayesian reasoning is particularly well-suited for addressing Hempel’s Raven Paradox, as it helps us explore whether the so-called paradoxical conclusion is truly paradoxical or simply a psychological illusion rooted in our subjective beliefs. (Urbach and Howson 2006)\n(Urbach and Howson 2006), demonstrates an elegant example of how Bayesian framework can provide a detailed analysis of the paradox.\nLet \\(H\\) denote the hypothesis “All ravens are black”, \\(P\\) a suitable probability function, \\(\\theta\\) the probability of a randomly chosen raven being black, \\(R\\) an observation of a black object, \\(\\overline{R}\\) an observation of a non-black object, \\(R\\) an observation of a raven, \\(\\overline{R}\\) an observation of a non-raven and \\(\\overline{R} \\overline{B}\\) for example an observation of a black raven.\n\\(P(H|\\overline{R} \\overline{B})\\) represents our belief regarding how likely it is that the \\(H\\) is true, given we observe a non-black non-raven, e.g. a white shoe. According to Bayes’ Theorem\n\\[\\begin{aligned}\n    P(H|\\overline{R} \\overline{B}) & = \\frac{P(\\overline{R} \\overline{B}|H) \\cdot P(H)}{P(\\overline{R} \\overline{B})} \\\\\n    & = P(H) \\cdot \\frac{P(\\overline{R} \\overline{B}|H)}{P(\\overline{R} \\overline{B})}\n\\end{aligned}\\]\nThe ratio between \\(P(H|\\overline{R} \\overline{B})\\) and \\(P(H)\\) can be seen as a measure for confirmation. This results in following analysis under some plausible assumptions:\n\\[\\begin{aligned}\n    \\frac{{}P(H|\\overline{R} \\overline{B})}{P(H)} & = \\frac{P(\\overline{R} \\overline{B}|H)}{P(\\overline{R} \\overline{B})} \\\\\n    & = \\frac{P(\\overline{B}|H)}{P(\\overline{R} \\overline{B})} \\\\\n    & =\\frac{P(\\overline{B})}{P(\\overline{R} \\overline{B})} \\\\\n    & = \\frac{P(\\overline{B})}{P(\\overline{R}|\\overline{B}) \\cdot P(\\overline{B})} \\\\\n    & = \\frac{1}{P(\\overline{R}|\\overline{B})}\n\\end{aligned}\\]\nAt first glance, some of the derivation steps may seem unclear due to the implicit assumptions involved. Let’s break them down:\n\nStep (3) essentially applies Bayes’ Theorem.\nStep (4) assumes that the probability of encountering a non-black non-raven (( )) is negligibly close to the probability of encountering any non-black object (()). This assumption seems reasonable, as there are far more non-black objects than ravens in general. While the equality used here is for simplicity and isn’t formally precise, it doesn’t affect the overall point of this analysis.\nIn step (5), it’s assumed that the probability of an object being non-black given that (H) is true is again very close to the prior probability (P()), which is justified by the overwhelming ratio of non-black things to ravens.\n\nUnder the assumptions mentioned, we come to the conclusion, that the ratio between \\(P(H|\\overline{R} \\overline{B})\\) and \\(P(H)\\), which is considered to be a measure for confirmation, is roughly equal to \\(\\frac{1}{P(\\overline{R}|\\overline{B})}\\). Another plausible assumption given the world we live in is that there are overwhelmingly more non-ravens than raven so it is safe to assume that \\[\n\\frac{1}{P(\\overline{R}|\\overline{B})} \\approx \\frac{1}{1 -\\epsilon} &gt; 1,\n\\] where \\(\\epsilon\\) is a very small number. As a consequence, this series of derivations under some plausible assumptions leads us to the result, that the observation of a white shoe does indeed confirms the hypothesis \\(H\\), that all ravens are black, but the degree of confirmation is almost negligible.\nTo understand why, consider the vast number of non-black, non-raven objects in the world. Observing one more of these objects, like a white shoe, barely shifts our confidence in the hypothesis, simply because such observations are so common. In Bayesian terms, while the likelihood ratio slightly favors ( H ), the weight of this evidence is lessened by the large number of similar observations. It’s like adding a single drop of water into an ocean, the water level technically rises, but it isn’t noticeable. This minimal confirmation highlights a crucial aspect of Bayesian reasoning: not all evidence is equally impactful. Although technically supportive, evidence like the white shoe is weak because it doesn’t address the core of the hypothesis. What truly matters is evidence directly tied to ravens themselves—seeing a black raven, for instance, offers much stronger support. In this regard, (Urbach and Howson 2006) ‘resolves’ the paradox by showing that it isn’t truly a paradox, while also shedding light on why it might seem like one to us."
  },
  {
    "objectID": "posts/raven-paradox/index.html#various-attempts",
    "href": "posts/raven-paradox/index.html#various-attempts",
    "title": "Hempel’s (Raven) Paradox: An Introduction",
    "section": "Various Attempts",
    "text": "Various Attempts\nOne approach that complements the Bayesian perspective is Patrick Maher’s use of Carnap’s theory of inductive probability (Maher 1999). Maher builds on the idea that observing non-ravens can indeed confirm the hypothesis, but in a way that differs from the intuitive, and often misleading, assumption that non-ravens inform us about the color of ravens. Instead, by reducing the total number of potential counterexamples to the hypothesis, Maher shows that such observations still contribute, albeit indirectly, to confirming “All ravens are black.”. A counterexample to the hypothesis would be a raven that is not black. Observing any raven that is black doesn’t falsify the hypothesis and thus supports it, but observing non-ravens, like white shoes or red apples, plays a different role. Even though a white shoe, for example, isn’t a raven and doesn’t tell us directly about raven colors, it still reduces the number of possible objects that could serve as a counterexample to the hypothesis. The more non-raven objects we observe, the more potential items we rule out as being non-black ravens. By systematically observing non-black, non-raven objects, we narrow the space of possible counterexamples that could disprove the claim that all ravens are black.\nAnother interesting attempt to resolve the paradox appeared on a single-page article titled “The White Shoe is a Red Herring” (Good 1967):\n\nSuppose that we know we are in one or other of two worlds, and the hypothesis, H, under consideration is that all the crows in our world are black. We know in advance that in one world there are a hundred black crows, no crows that are not black, and a million other birds; and that in the other world there are a thousand black crows, one white one, and a million other birds. A bird is selected equiprobably at random from all the birds in our world. It turns out to be a black crow. This is strong evidence (a Bayes-Jefrreys-Turing factor 2 of about 10) that we are in the second world, wherein not all crows are black. Thus the observation of a black crow, in the circumstances described, undermines the hypothesis that all the crows in our world are black. Thus the initial premise of the paradox of confirmation is false, and no reference to the contrapositive is required\n\nThis thought experiment from (Good 1967) introduces an important point: the validity of Hempel’s first premise, Nicod’s criterion, depends heavily on the background information we consider. By imagining two possible worlds, the example shows that observing a black crow can sometimes weaken the hypothesis, depending on the specific context. This challenges the traditional understanding of Hempel’s paradox, suggesting that confirmation may be more context-dependent than originally thought.\nAnother example of such a configuration of background information is found in (Rosenkrantz 1977). Consider the following scenario: Three men, \\(M_1\\), \\(M_2\\), and \\(M_3\\), are wearing three different hats, \\(\\textit{Hat}_1\\), \\(\\textit{Hat}_2\\), and \\(\\textit{Hat}_3\\). They enter a bar, leave their jackets and hats at the coat check, have some drinks, and become drunk. When it’s time to leave, they each pick a hat at random. Let \\(H_\\textit{hat}\\) denote the hypothesis that each man picks a hat that does not belong to him. According to Nicod’s criterion, if we learn that \\(M_1\\) picked \\(\\textit{Hat}_2\\), this evidence supports the hypothesis \\(H_\\textit{hat}\\) since \\(M_1\\) has chosen a hat that isn’t his. If we also learn that \\(M_2\\) picked \\(\\textit{Hat}_1\\), Nicod’s criterion would again suggest this supports the hypothesis, as \\(M_2\\) has also picked a hat that does not belong to him. However, if \\(M_1\\) and \\(M_2\\) had picked each other’s hats, we would implicitly know that \\(M_3\\) picked his own hat, meaning the hypothesis is actually false. In this case, the evidence of \\(M_2\\) picking \\(\\textit{Hat}_1\\) would actually falsify the hypothesis rather than support it, contrary to what Nicod’s criterion would suggest.\nHowever, Hempel responds in his article “The White Shoe: No Herring” (Carl G. Hempel 1967), by arguing that the paradox should be analysed without reference to background information. He insists that the consideration of additional factors compromises the logical clarity of the paradox. In his view, any observation of an A that is B must universally confirm the hypothesis, regardless of what we know about the world. Hempel’s response calls for a stricter interpretation of confirmation, where the process is kept pure and uninfluenced by external considerations. This debate over whether to include or exclude background information raises a broader question: can we really evaluate any hypothesis in isolation from its context? By questioning the universal applicability of Nicod’s criterion, arguments such as “The White Shoe is a Red Herring” highlight potential limitations in the original formulation.\nAnother alternative thought-provoking approach to address the paradox is Quine’s argument of natural kinds (Quine 1970). The Raven Paradox, as outlined by Hempel, relies on the assumption that objects can be neatly classified into categories like “raven” and “black” in such a way that a white shoe could logically confirm the hypothesis “All ravens are black”. However, Quine argues that not all groupings of objects are equally valid when it comes to inductive reasoning. Instead, he claims that categories that reflect the real, inherent structure of the world are essential to making sense of inductive generalizations, which he calls “natural kinds”. Quine’s notion of natural kinds provides a way to dismiss the paradox as a problem that arises from a misunderstanding of how we should group objects for the purposes of confirmation. In the case of the Raven Paradox, Quine would argue that the category of “non-black non-ravens” (e.g., white shoes) is not a natural kind, and thus observations of such objects cannot genuinely confirm or disconfirm a hypothesis about ravens. The paradox arises, in Quine’s view, because it mistakenly treats irrelevant categories as if they were meaningful for inductive reasoning.\nThis perspective is closely related to Goodman’s grue-emerald argument (Goodman 1983), which also questions the validity of certain categories in inductive logic. Goodman’s grue hypothesis highlights the problem of arbitrary classifications. By defining “grue” as something that is green before time \\(T\\) and blue after, we find that the same observations can support two contradictory hypotheses: “All emeralds are green” and “All emeralds are grue”. Both Goodman and Quine converge on the idea that not all predicates are equally valid when it comes to inductive generalizations. Goodman claims that certain predicates, such as ‘grue’, are artificially constructed and lack the intuitive appeal of others, such as ‘green’. Quine extends this notion by arguing that only natural kinds - those that reflect real divisions in nature - should be used to formulate hypotheses and confirm them by observation. According to Quine, the Raven Paradox, like Goodman’s grue problem, exposes how our reliance on artificial categories can distort inductive reasoning. In the Raven Paradox, the category of “non-black non-ravens” is treated as relevant for confirming a hypothesis about ravens, but Quine would argue that this category is too arbitrary to support any meaningful inference. The same issue appears in Goodman’s grue hypothesis: the classification of emeralds as “grue” is not based on a natural kind but on an arbitrary, division."
  }
]